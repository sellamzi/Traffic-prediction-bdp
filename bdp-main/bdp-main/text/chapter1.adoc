[[ch:big-data]]
= Big Data Processing
// was: Distributed data processing

== The Rationale for Distributed Processing

=== Data Science in the small
'`Data Science in the small`' refers to the entire analytical lifecycle performed on datasets that are constrained in volume and complexity to a degree that they can be managed, processed, and analyzed on a single machine.
This paradigm contrasts with '`Data Science in the large`', which necessitates distributed computing frameworks to handle petabyte-scale data. 

In the '`small data`' context, the focus is on the efficient use of in-memory computation and the statistical richness of analytical methods. 
The typical toolset includes programming languages like Python with its scientific computing stack (Pandas, NumPy, Scikit-learn) or R, where the entire dataset is loaded into RAM for iterative exploration, modeling, and visualization. 
The process is often characterized by a more interactive and less formalized pipeline, common in academic research or initial proof-of-concept stages, where the velocity and variety of data do not yet impose architectural constraints.

=== Necessity of Big Data processing
While '`Data Science in the small`' is powerful for many analytical tasks, its methods and tools fundamentally break down when the volume and velocity of data exceed the physical limits of a single machine.
The transition to '`Big Data processing`' is not a choice of preference but a _necessity_ dictated by scale. 
Once a dataset can no longer fit into a computer's RAM, or even onto its local disk, the single-machine paradigm becomes untenable.
This threshold forces a shift from vertically scaling a single powerful server to horizontally scaling across a distributed cluster of machines.

This is not a theoretical or distant boundary, as many modern datasets already far exceed single-node capacity.
Consider these examples, with some data estimates (2025) to reflect their scale:

. *The 1000 Genomes Project*: The https://www.internationalgenome.org/1000-genomes-summary[1000 Genomes Project] has created a library of human DNA.
A high-quality digital blueprint of a single person's genome takes up about 100 GB.
The latest, more detailed data of thousands of indviduals has pushed the total size well above 250TB.

. *The Internet Archive*: 
Its public-facing https://web.archive.org/[Wayback Machine] now holds over 100 petabytes of data, archiving a significant portion of the public web and demonstrating a problem of both immense volume and extreme variety.

. *Wikipedia*: While the compressed text of all English articles is a manageable ~25 GB, the full dataset including all languages, edit histories, and media files is in the tens to low hundreds of terabytes, presenting a significant processing challenge for holistic analysis.

These examples illustrate that the constraints necessitating Big Data processing are common in science, industry, and knowledge preservation. 
However, it's important to correctly identify _when_ this transition is required.

The blog post https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb[You Are Not Google] may serve as a  pragmatic check.
It argues that many organizations adopt complex distributed systems like those pioneered by Google _without_ having Google-scale problems.
The text therefore calls for disciplined engineering: one should only adopt the complexity of a distributed processing framework when the scale of the data genuinely demands it, not as a premature optimization or in imitation of hyperscale companies whose operational realities are fundamentally different.

[quote, Oz Nova, You Are Not Google]
____
If you’re using a technology that originated at a large company, but your use case is very different, it's unlikely that you arrived there deliberately; no, it's more likely you got there through a ritualistic belief that imitating the giants would bring the same riches.
____

== The 5 '`Vs`' of Big Data
The transition to distributed data processing is a direct consequence of the limitations of single-machine systems when confronted with data characterized by three properties, often called the '`Vs`':

=== Core Characteristics

Volume::
  The total size of the dataset exceeds the storage capacity of a single machine.
  For example, petabyte-scale datasets from scientific instruments or web crawlers cannot be stored on a single node's disk array.

Velocity::
  The rate of data generation and ingestion exceeds the processing or I/O capacity of a single machine.
  For instance, financial trading systems or sensor networks can generate millions of events per second.

Variety::
  Data is generated in heterogeneous formats, from structured relational data to semi-structured (JSON, XML) and unstructured (text, images) data.
  While not a direct cause for distribution, processing this variety at scale often requires flexible, parallel computation frameworks.

Conforming to the three Vs is to move from vertical scaling (increasing the resources of a single machine) to horizontal scaling (adding more machines to a cluster).
This necessitates a distributed, shared-nothing architecture, where each node has its own CPU, memory, and storage.
This means that the primary bottleneck shifts from CPU performance to disk and network I/O.

=== Business Context
The initial 3 Vs already mentioned describe the technical problem domain of Big Data.
Over time, however, it became apparent that managing the technical challenges was insufficient.
The utility of the data itself presented further dimensions.
This led to the addition of two more Vs, which address qualitative and business aspects:

Veracity::
  This refers to the quality, trustworthiness, and uncertainty of the data.
  It addresses the '`messiness`' of real-world data, including:
  * Noise and Bias: Inaccuracies, duplicates, and systemic skews.
  * Ambiguity: Data that can be interpreted in multiple ways.
  * Incompleteness: Missing values or records.
  High-velocity data from sources like social media or IoT sensors often has low veracity. 
  A significant part of any data processing pipeline involves cleaning and validating data to improve its veracity.

Value::
  This refers to the utility or economic benefit derived from the data. 
  Raw data itself has no intrinsic value: its value is realized through analysis that leads to actionable insights. 
  This V forces the question: "`Is the significant cost of storing and processing this data justified by the benefits it provides?`"
  It links the technical data processing exercise to strategic business outcomes, such as cost reduction, process optimization, or the creation of new products.

.The five Vs of Big Data <<fiveVs>>.
image::big-data-5v.png[5 Vs, 400]

=== Big Data Processing Architecture
The relationship between the five Vs of Big Data and Big Data processing is one of cause and effect. 
The Vs define the characteristics and challenges of the data (the problem), while '`Big Data processing`' encompasses the set of architectural patterns, systems, and algorithms designed to overcome these challenges (the solution).

Each V directly dictates specific requirements for the processing systems.

.Volume → distributed storage, parallel processing
Problem: the dataset is too large to fit on a single machine's disk.
Processing requirement: The system must be distributed. This leads to two fundamental needs:

. Distributed storage: data must be partitioned into blocks and spread across a cluster of machines (e.g., HDFS, cloud object stores like S3). 
Fault tolerance is managed through replication of these blocks.

. Parallel processing: computation must be executed in parallel across the nodes of the cluster.
The processing framework must be able to schedule tasks on different nodes to operate on local subsets of the data simultaneously. 
The MapReduce paradigm, and its evolution in systems like Apache Spark, is a direct answer to this requirement.

.Velocity → real-time ingestion, stream Processing
Problem: data arrives continuously and at a high rate, making it impossible to collect and store it all before processing begins. 
The value of the data may also decay quickly with time.

Processing requirement: The system must handle data in motion. 

* Real-time ingestion: a high-throughput, durable '`front door`' is needed to absorb the high-velocity data streams without loss. 
This is the role of distributed message brokers like Apache Kafka. 
They buffer incoming data and make it available for consumption by processing engines.

* Stream processing: the processing model must shift from operating on a static, bounded dataset (batch processing) to operating on a continuous, unbounded stream of events. 
This requires capabilities for windowing, state management, and handling event time. 
Apache Flink and Spark Streaming are direct solutions to this problem.

.Variety → flexible data models and schemas
Problem: the data does not conform to a single, rigid relational schema. 
It can be structured (tables), semi-structured (JSON, XML), or unstructured (text, images, audio).

Processing requirement: The system must be able to ingest, process, and store these heterogeneous data types.

* Schema-on-read: Unlike traditional data warehouses that enforce a strict schema on data as it is written (schema-on-write), big data systems often use a '`schema-on-read`' approach. 
Data is stored in its raw format, and structure is imposed only when it is queried. 
This provides the flexibility to handle diverse and evolving data formats.
This is related to the process of '`mapping down`' data that we discuss in, for example, <<sec:pair-rdd-motivation>>.

* Support for complex data types: Processing frameworks must have native support for complex, nested data structures (like JSON objects or arrays) and user-defined functions (UDFs) to parse and manipulate unstructured data.

While the first three Vs dictate the core system architecture, Veracity and Value dictate the logic within the processing jobs.

.Veracity → data cleansing
Problem: the raw data is often unreliable, containing errors, duplicates, missing values, and biases.

Processing requirement: processing pipelines must include explicit steps for data cleansing, validation, and enrichment. 
This can involve filtering out invalid records, deduplicating entities, inferencing missing values, and normalizing data to improve its quality before analysis. 
Low veracity increases the computational workload dedicated to these preparatory steps.

.Value → purposeful analytics
Problem: processing vast amounts of data is computationally expensive. 
The effort is wasted if it does not yield useful insights.

Processing requirement: The processing must be purpose-driven. 
This influences algorithm selection and overall pipeline design. 
The goal is not merely to process the data, but to transform it in a way that extracts a specific, valuable signal. 
For example, a batch job might be designed to train a machine learning model, or a streaming job might be designed to detect fraudulent transactions in real-time. 
The '`Value`' requirement ensures that the technical processing is aligned with a functional or business objective.

In summary, the Vs provide a complete specification for a big data processing system. 
Volume and Velocity define the scale and speed requirements (the '`non-functionals`'), Variety defines the required data model flexibility, and Veracity and Value define the analytical purpose and data quality constraints that the processing logic must satisfy.


== Core Principles of Distributed Processing
The transition from single-machine computation to distributed processing is driven by the physical limitations of a single node in terms of CPU, memory, and I/O. 
To effectively harness the power of a cluster of interconnected machines, a set of fundamental architectural principles must be employed. 
This section outlines these core concepts, from the initial strategy of partitioning data and executing tasks in parallel, to the critical requirement of achieving data locality and ensuring fault tolerance in the face of hardware failure.

=== Data Parallelism and Locality
To achieve scalability, distributed systems and big data processing frameworks like Spark and Flink employ data parallelism.
This is a strategy where the same operation is applied concurrently to different subsets of the data. 
// The performance of this strategy, however, is entirely dependent on a second, critical principle: data locality. 

// This section explains how the mantra of "moving computation to the data, not the data to the computation" is the essential mechanism for minimizing network overhead and enabling efficient parallel execution across a cluster.


A simple but good analogy for data parallelism would be translating a 1000-page book.
It would be slow for one translator, but if you tear the book into 10 sections of 100 pages and give one section to each of the 10 translators, they can all work simultaneously, and the job would be done much sooner.
This means that the operations have to '`follow`' the data.
The more formal term for this principle is _data locality_, which is often expressed as: "`Move computation to the data, not the data to the computation`".
This is the core tenet for achieving _efficient_ data parallelism. 
Instead of pulling massive, distributed data partitions over the network to a central compute node, the framework sends the (much smaller) code for the operation to the worker nodes where the data partitions already reside.
By having the operations follow the data, the system minimizes its most significant performance bottleneck: network I/O.

// The link is data locality. Because the data is already distributed across the cluster, the processing framework (like Spark) can send the computation to the nodes where the data already lives. This avoids the massive bottleneck of moving terabytes of data over the network to a central processing unit, which is the key to making data parallelism effective at scale.


=== Distributed Storage
Distributed storage solves two distinct but related problems.

. The '`Big File`' problem, i.e., storage necessity.
This is the original and most intuitive reason. 
If you have a single 100 TB file from a scientific simulation or a satellite image, you have no choice but to split it and store it across multiple machines. 
Files are partitioned into smaller, fixed-size blocks or chunks (e.g., 128MB or 256MB).
A master node maintains the metadata mapping a file to its constituent blocks and their locations.
These blocks are then distributed across the data nodes of the cluster.
To handle node failures, fault tolerance is achieved through replication. 
Each block is replicated a set number of times (typically 3), and these replicas are stored on different nodes, ideally in different physical racks to protect against rack-level power or network failures.
The Hadoop Distributed File System (HDFS) is a canonical implementation of this model.

. The '`Fast Processing`' problem, i.e., performance necessity.
This is the more common and powerful use case in modern data analytics. 
You might have a 10 TB dataset composed of millions of smaller log files. 
While this dataset could technically be stored on a single large hard drive, doing so would create a massive I/O bottleneck.
Any query you run will be limited by the read speed of that single disk and the processing power of that single machine.
By distributing those files across 100 machines (100 GB per machine), you enable data parallelism. 
Now, a query can be executed by 100 machines simultaneously, each reading its small local portion of the data. 
This means you get 100x the disk I/O throughput and 100x the processing power, turning a job that might take hours into one that takes minutes.
Distributed storage therefore is the physical foundation that makes data parallelism possible and efficient.

The fault tolerance mechanism of a distributed storage system is independent of the data's size or structure. 
The system's fundamental task is to partition data into blocks and replicate those blocks for resilience. 
This process is applied uniformly, whether it's breaking down a single massive file that's too large for one node or storing millions of smaller files to enable data parallelism. 
In either case, fault tolerance is a feature of the storage layer itself, which is indifferent to the nature of the data it protects.

== Inherent Challenges of Distribution

=== Shared-Memory Parallelism
In shared-memory parallelism, all processing units (e.g., the cores in a multi-core CPU) have direct and uniform access to a common main memory. 
Unlike in distributed systems, the data isn't physically partitioned.
Instead, a single copy of the entire dataset resides in RAM.

Partitioning the work is typically done by assigning different threads, running on different cores, to process different logical segments of the same in-memory data structure. 

A common example is processing a large array, of which a single copy exists in RAM.
The program launches multiple threads, and each thread is assigned a different range of indices to work on (e.g., Thread 1 processes indices 0-999, Thread 2 processes 1000-1999, etc.).
Because all threads are accessing the same memory locations, communication is extremely fast, occurring at memory-bus speeds. 
The primary programming challenge is therefore not latency but _synchronization_: mechanisms like locks or atomic operations must be in place to prevent threads from corrupting the shared state. 
Furthermore, the failure model is typically '`all-or-nothing`': a hardware or OS fault brings down the entire machine at once.

Distribution introduces important concerns beyond what we have to worry about when dealing with parallelism in the shared memory case: latency and partial failures.

=== Latency
While distributing data and computation solves the physical limitations of a single machine, it introduces network latency as the primary performance bottleneck. 
In a shared-memory system, communication between parallel tasks happens at the speed of RAM. 
In a distributed system, any communication between worker nodes must traverse the network, which is orders of magnitude slower.

This latency is incurred whenever data is moved, which happens for two main reasons:

. when intermediate results are exchanged between worker nodes, and

. when a final result is computed by combining the results from each node.

To understand the severe cost of this communication, it is useful to compare the latencies of different data access operations.
The following table puts these latencies into perspective by scaling them to a human-understandable timeframe by multiplying all durations by a billion.
If we imagine that accessing a piece of data from the CPU's L1 cache takes half a second, the relative times for other operations are staggering: a main memory reference would take 100 seconds, sequentially reading 1MB from RAM 3 _days_, a network roundtrip in the same datacenter 6 days, sequentially reading 1MB from disk 8 _months_, and sending a network packet from California to Belgium and back about 5 _years_.

This comparison makes the performance hierarchy clear: accessing data from memory is vastly preferable to accessing it from disk, but both are _dramatically_ faster than sending it over the network. 
This immense cost is why the central design principle of high-performance distributed systems is data locality: move the computation to the data, not the data to the computation.

The primary strategy for mitigating latency is to minimize network communication. 
This is a shared responsibility, addressed through both deliberate program design by the developer and automatic optimizations performed by the runtime system.

A developer can reduce communication by favoring operations that are embarrassingly parallel (like `map` or `filter`), which process data partitions independently without needing to exchange information. 
When global coordination is required, the goal is to minimize the volume of data that must be exchanged. 
This is achieved through for example pre-aggregation on each worker node before communicating results so that the amount of data to be communicated is significantly reduced.

The distributed data processing system can automatically apply several powerful optimizations to also reduce communication overhead by always attempting to run a task on the same machine where its data partition resides (i.e., data locality), fusing chains of embarassingly parallel operations into a single stage, avoiding the need to write and re-read intermediate results (<<sec:pipelining, pipelining>>), and other optimizations such as tree-like aggregation patterns and the use of efficient serialization formats and network protocols to reduce the overhead of data transfer.

Although careful program design and sophisticated runtime optimizations can drastically mitigate latency, they cannot entirely eliminate it. 
The physical cost of network communication remains a fundamental constraint that shapes the design of both distributed systems and the programming models used to interact with them.

[[sec:partial-failures]]
[[sec:fault-tolerance]] 
=== Partial Failures and Fault-Tolerance
In any system, failures are inevitable: machines crash, networks partition, messages drop. 
In a single computer, a hardware failure is typically total, meaning that the entire system stops.
In a distributed system, however, one server can crash or a network link can break while the rest of the components continue to run.
This creates a state where the system is neither fully operational nor fully failed.
We therefore call this a partial failure.

Fault tolerance is the property that enables a system to continue operating correctly, potentially at a reduced level of performance, even in the event of partial failures.
It is a fundamental requirement for building reliable distributed systems.

The programmer should not have to write code to handle partial failures.
Instead, the framework is expected to manage this automatically. 
This is achieved through a multi-layered fault tolerance strategy that ensures both data durability and computational correctness.

The foundation of fault tolerance is ensuring the source data is never permanently lost. 
The distributed storage system (like HDFS) handles this through replication. 
By storing multiple copies of each data block on different nodes, the system guarantees that data remains available for computation even if a machine fails.

The compute layer is responsible for recovering the in-progress computation. 
The mechanism depends on the nature of the job.
For stateless or batch jobs, resilience is often achieved by '`replaying`' certain operartions based on <<sec:lineage, lineage>>. 
Lineage is a DAG that records operations and their dependencies, and allows re-execution of these operations whenever data is lost or unavailable.

For stateful streaming jobs, resilience relies on checkpointing.
Frameworks like Flink periodically save a consistent snapshot of the application's state to durable storage.
If a failure occurs, the application can be restarted from the last successful checkpoint.

To ensure that re-executing a failed task doesn't produce incorrect results (e.g., counting the same transaction twice), two principles are essential: idempotency and consistency.
Operations are idempotent when executing them multiple times has the same effect as executing them once.
For consistent execution of stateful operations, the data processing framework should provide guarantees like at-least-once or exactly-once semantics for these operations to define how state is managed across failures, preventing data loss or duplication.

==== Mean Time Between Failures
Mean Time Between Failures (MTBF) is a measure of a system's reliability, representing the average time it operates before a failure occurs.
The MTBF of a compute node is the average time between two failures.

Formally, it expresses the average expected time between two consecutive failures of a node.

latexmath:[\text{MTBF}=\frac{\sum(\text{downtime start}-\text{uptime start})}{\text{number of failures}}]

In distributed systems, however, MTBF applies not to one monolithic system, but to each independent component or node (machine, VM, container, disk, etc.) that can fail.
In the setting of data-parallel processing systems, each worker node has an individual MTBF.
When you have many worker nodes, the _system-wide_ MTBF is much lower than the MTBF of individual worker nodes, because _any_ node failure can cause a partial (system) failure.
The intuition behind this is that with N independent components, each capable of failing, the chance that some failure occurs in the next instant is roughly N times higher.
As a consequence, failures in large distributed systems should be considered as normal events rather than rare anomalies.

====
Suppose each node has an MTBF of 1 year, which is 8760 hours.
Also suppose that your cluster consists of 1000 nodes.
Then the MTBF of a cluster (i.e., the average time between _some_ node failing) is only 8.76 hours.
====

MTBF is a critical input for designing a system's fault tolerance strategy, as it provides a statistical estimate of how often failures can be expected.
This failure frequency dictates the configuration of the system's recovery mechanisms. 
At its core, this represents a fundamental trade-off between the overhead of fault prevention and the cost of failure recovery.
If the system saves its state (e.g., via checkpointing or logging) very often, the overhead during normal operation will be high, slowing down overall progress.
However, if a failure occurs, the amount of lost work is minimal, and recovery is fast.
If the system saves its state rarely, the overhead is low, and the system makes fast progress during normal operation. 
However, a failure may force the system to re-do a large amount of work, leading to a long recovery time.

The optimal strategy is a balance between these two extremes.
The goal is to configure the fault tolerance mechanism so that the system can make sufficient progress between the inevitable failures, minimizing the total time lost to both the preventative overhead and the recovery process.

// While latency and partial failures are indeed the core ones, other challenges can be seen as derivatives or secondary:
// Skew and load balancing: uneven data partitions cause latency and resource waste.
// State management: keeping distributed state consistent and recoverable.
// Scheduling and elasticity: efficiently assigning resources as workloads change.
// Determinism and reproducibility: crucial for debugging and correctness.

== Course overview
In this course we study topics, tools, frameworks, architectures, and technologies for Big Data processing. 
We are not so much interested in compute-intensive systems, where the prime challenge consists of parallelizing some very large computation. 
We are more interested in _data-intensive_ applications, where the main concerns are things like sorting and processing large data volumes, managing changes to data, ensuring consistency in the face of failure and concurrency, and making sure services are highly available [<<kleppmann>>].

This course focuses on the architectures and frameworks for distributed systems, not on specific applications like machine learning or data science.
The primary objective is to cover the underlying infrastructure for large-scale data processing, including distributed data-parallel processing frameworks, real-time streaming and complex event processing, and the low-coupling integration of these components using message brokers.
While the principles covered are foundational for building scalable machine learning and data science systems, the course's focus remains on the infrastructure itself.

=== Topics and technologies
The topics we will cover are the following:

* Batch Processing
* Declarative Data Processing
* Stream Processing
* Complex Event Processing
* Data Processing Ecosystem and Architecture

We will explain these topics by looking closer at the following frameworks and tools:

* Apache Spark
* Spark SQL
* Spark Streaming
* Apache Flink
* Apache Kafka

Why do we look at Apache Spark, Flink and Kafka frameworks from up close, even when some parts of these are not directly used anymore (Spark RDDs), and even when many high-level cloud services exist in which you can perform Big Data Processing and querying (e.g., Google Bigquery) in which these or similar lower-level technologies are hidden?

Three reasons:

. It is important to know the basics, and the origin and history, to better understand modern technology and frameworks.

. Even when using cloud services or high-level frameworks, maybe with nice user interfaces, it is important to understand what is behind all it.
It will make you a better developer, even when using these high-level solutions.

. Many large companies (for example https://www.kai-waehner.de/blog/2025/06/09/how-openai-uses-apache-kafka-and-flink-for-genai/[OpenAI]) heavily rely on Big Data technologies.
It is therefore interesting to know and understand how these companies can operate at scale. 
At the same time, it will make your CV attractive for these kind of companies, and basically any technology company, if you know and understand these technologies.


''''

[[sec:exercises-1]]
== Exercises

The purpose of this first exercise session is to get you up and running, in order for you to be able to run the examples, try the exercises, and develop and run the project.

The setup uses free and widely available software and tools, available on all the major operating systems.
We will explain a particular setup involving git, Visual Studio code, Scala, and Docker Desktop.
You are free to deviate from the actual setup, but then you are on your own for figuring things out. 

=== Exercise 1: Visual Studio Code and Docker (Desktop).

Install git, vscode and Docker, for example by following these links:

** https://git-scm.com/[git]
** https://code.visualstudio.com/download[Visual Studio Code]
** https://https://www.docker.com/products/docker-desktop/[Docker Desktop]

****
If you are running this in a guest OS on a virtual machine, then you cannot run Docker Desktop, because Docker Desktop uses a VM itself for hosting the containers that relies on ``actual'' (i.e., hardware) virtualization infrastructure. 
In this case, you can work with Docker engine (which is installed as part of Docker Desktop).
In what follows, you will then have to use the CLI to execute Docker commands (although that, even in Docker Desktop, we will often resort to the CLI as well).
****

In vscode, install the following extensions:

** https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-containers[Container tools]
** https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers[Dev Containers]
** https://marketplace.visualstudio.com/items?itemName=scalameta.metals[Scala (Metals)]
** https://marketplace.visualstudio.com/items?itemName=scala-lang.scala[Scala Syntax]


=== Exercise 2: BDP repo and containers

Register an account using your VUB email address on our https://gitlab.soft.vub.ac.be/[gitlab].

Request membership of the ``bdp-students-2025`` group by navigating to https://gitlab.soft.vub.ac.be/bdp-students-2025[the groups's page] and using the hamburger menu (three vertical dots) in the top-right.

Someone from the BDP team will approve your membership request shortly.
When this has happened, you can access the https://gitlab.soft.vub.ac.be/courses/bdp[repository for the course].

CAUTION: The BDP repo is a https://gitlab.soft.vub.ac.be/help/topics/git/lfs/_index.md[Git LFS repo].
Git Large File Storage (LFS) replaces certain large files (including  `.zip` file of large datasets in {dataset-dir}) with text pointers inside Git, that will be replaced with the actual large files when actually required.
git-lfs is already installed _inside_ the devcontainer, where the datasets (the actual large files) are also pulled and unzipped.
So, in theory you do not need git-lfs installed '`outside`' the containers to obtain the BDP repo to just run the containers, without committing (large files) to the repo.
However, if you want to experiment further with the BDP repo, and people (assistants) that have commit rights and commit to it, must install git-lfs to correctly work with large files.

  git lfs install

Open a console and clone the BDP repository. 
The repo will be downloaded to the current directory if you do not specify the local destination directory as optional parameter after the repo URL.

  git clone https://gitlab.soft.vub.ac.be/courses/bdp.git <optional-local-target-dir>

Provide your username and password when requested.

In vscode, open the `bdp` repository folder (trust the authors).

In the explorer you should now see a `.devcontainer` folder, several ``hello-...`` project folders, a ``.gitignore`` file, a ``docker-compose.yml``, a README, etc.

In the bottom left of the vscode window, click on the green ``><`` symbol ("Open a Remote Window").
Select "Reopen in Container" from the command menu.

Wait for the containers to come up.
This will take many minutes, maybe even longer than half an hour (grab a drink!).
It also involves a fair amount of data being downloaded, so don't do this if you are on a limited data plan. 
Click on ``show log`` to monitor the progress.

If the scrolling stops and you see something that resembles the output below, then (most probably) the initialization succeeded.

  [1353301 ms] [14:00:59] [127.0.0.1][05705670][ManagementConnection] New connection established.
  [1353312 ms] [14:00:59] Started initializing default profile extensions in extensions installation folder. file:///root/.vscode-server/extensions
  [1353427 ms] Port forwarding 59150 > 34329 > 34329 stderr: Connection established
  [1353521 ms] [14:00:59] Log level changed to info
  [1353704 ms] [14:01:00] ComputeTargetPlatform: linux-arm64
  [1353913 ms] [14:01:00] [127.0.0.1][3389d816][ExtensionHostConnection] New connection established.
  [1354005 ms] [14:01:00] [127.0.0.1][3389d816][ExtensionHostConnection] <344> Launched Extension Host Process.
  [1354239 ms] [14:01:00] Completed initializing default profile extensions in extensions installation folder. file:///root/.vscode-server/extensions
  [1356027 ms] [14:01:02] ComputeTargetPlatform: linux-arm64

Open a new terminal in vscode (click on the `+` symbol). 
Your prompt should look something like this:

  root@cfa1990433e0:/workspace#

You are now connected to your devcontainer, and the commands you enter at this point are executed inside your devcontainer.
If you are unsure, use the command `hostname` (Mac or Linux).
If it does not return a hex string like `cfa1990433e0` in the example above, but rather some local name, then you are _not_ connected to the devcontainer.

The `><` menu should now also indicate that your workspace is running in the devcontainer: `Dev Container: BDP @ desktop-linux`.

Go to the container view in Docker Desktop. 
You should see a group of containers under `bdp` that are all "`green`".
If ``bdp`` itself is not green, because some containers did not start correctly, then you could try to start them manually by "`pressing play`".
For example, it could be that the ``kafka-ui`` container did not start correctly because it timed out waiting on other containers it depends on.

.Successfully running containers.
image::successfully-running-containers.png[]

Familiarize yourself with Docker Desktop by clicking around, checking logs, etc. 

Go back to vscode, and click on ``><``. 
You can reopen the bdp folder locally (needed when errors occur), reopen the containers so that you can connect to the devcontainer again, close the remote connection, rebuild the container (takes a long time!), etc. 
Familiarize yourself with these options.


=== Exercise 3: Running some Scala

In the devcontainer(!) type `sbt new` and choose `Scala 2 seed template` (type kbd:[e]).

NOTE: Spark and many other technologies still rely on Scala 2, although they have plans to move to Scala 3.

Choose ``hello-scala`` as name.

Navigate to the ``Hello.scala`` class to inspect its contents.

Run the program:

  cd hello-scala
  sbt run

The first time you compile and run, sbt will fetch the necessary dependencies.
If you run the program a second time, it will be much faster with less additional output.

=== Exercise 4: Testing some containers

To ensure that the containers are up and running, we will execute some command in each of them.

Navigate to the ``hello-kafka`` directory inside the ``examples`` directory in your devcontainer.
Execute ``sbt run`` and check whether this completes successfully.
In Docker Desktop, click on ``8082:8080`` in the ``ports`` column of container ``kafka-ui``. 
You should get a page that indicates a running cluster with "`1 of 1`" brokers online.
If you click on ``topics``, you should see a ``hello-input-topic`` and a ``hello-output-topic``.
You can inspect these topics to see the messages that were just produced and consumed.
In the code and the execution logs you will see the same message appear.

Navigate to `hello-spark` and again perform and check the output of ``sbt run``.
Note that some logging may be channeled to std.error, but this is not a problem.
In Docker Desktop, click on port ``8080:8080`` of container ``spark-master`` and you should see a page representing its status, with 0 running and completed applications.
Now perform the following commands:

  sbt clean assembly
  spark-submit --class example.Hello --master spark://spark-master:7077 target/scala-2.13/myProject.jar

Refresh the Spark Master status page, and you should see that one application completed.

Also perform `sbt run` in `hello-flink-batch` and verify the output is successful.

Next, execute the following command in the console to submit a Flink job that is included as an example in the Flink distribution.

  flink run -m flink-jobmanager:8081 /opt/flink/examples/streaming/WordCount.jar``

Inspect Flink's job manager on port `8081:8081`, and check that there are two available task slots and one task manager. 
You should also see that there are 0 running jobs, and one finished.
When clicking on `Completed Jobs` in the left menu, the `WordCount` job should be listed.

''''

Although at this point we have not tested every last bit of code and functionality, it is safe to assume that, if you have made it this far without errors, your containers are correctly set up and functioning.
This means you are ready for the real work.

The goal of the `Hello` test programs and the example commands in the `README`s is to give you starting points for the upcoming exercises.
The idea is that you can copy these project folders and then adapt the source code to solve exercises.
This, in combination with the test programs that show how the different frameworks interact (e.g., a Flink job producing to and consuming from Kafka topics) should enable you to tackle the project.

=== Exercise 5: Creating a Wilma account
In later exercise sessions, we will be using Isabelle, a cluster computer hosted by SOFT.
You will also need to use this server for your big data project.

For us to be able to grant you access to the server, you need to have a Wilma account. 
These accounts are also used in the computer rooms at InfoGroep. 
If you do not have a Wilma account yet, you need to create one at https://wendy.vub.ac.be/accounts/en. 

IMPORTANT: Without a Wilma account, we will not be able to grant you access to the server and you will not be able to access the server for your project.

Hence, if you fail to create your Wilma account on time, you may not be able to benchmark for your project!
Therefore, create your account today if you do not have one yet.

If you forget your password, follow the registration procedure again and the password of your VUB account will become the password of your Wilma account.
